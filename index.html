script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<!DOCTYPE html>
<html lang="en">

<head>
<title>CoMeDi: Context and Meaning - Navigating Disagreements in NLP Annotations</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- Prevent caching -->
<META HTTP-EQUIV="Pragma" CONTENT="no-cache">
<META HTTP-EQUIV="Expires" CONTENT="-1">
<link href="bootstrap/css/bootstrap.min.css" rel="stylesheet" media="screen">

<style>
/* Customize container */
@media (min-width: 968px) {
  .container {
    max-width: 968px;
  }
}
.container-narrow > hr {
  margin: 30px 0;
}
/* Customize dropdown menu */
.dropdown {
 cursor: pointer;
}
.dropdown sup {
 color: rgb(66, 139, 202);
}
.dropdown sup:hover, .dropdown sup:focus {
    color: rgb(42, 100, 150);
    text-decoration: underline;
}
.dropdown-menu {
  min-width: 500px;
  left: -200px;
}
.dropdown-menu li {
  margin-bottom: .5em;
  /*border-top-style:solid; padding-left:10px;*/
}

ol.answers {
  padding-top: 0;
  margin-bottom: 1em;
}

.task-example {
  border: 2px solid #ccc;
  padding: 1em;
  margin: 1em;
}

.ent {
  font-weight: bold;
  color: #686;
}

</style>
</head>

<body data-spy="scroll" data-target="#navbar" data-offset="70" onload="load()">

    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">CoMeDi</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="#speakers">Invited Speakers</a></li>
	    <li><a href="#program">Program</a></li>
            <!--li><a href="#call">Call</a></li-->
            <li><a href="#task">Shared Task</a></li>
            <!--li><a href="#leaderboards">Leaderboards</a></li-->
            <!--li><a href="#dates">Dates</a></li-->
            <li><a href="#organizers">Organizers &amp; Committee</a></li>
           </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="https://coling2025.org/">COLING 2025</a></li>
            <!--li><a href="../navbar-static-top/">Static top</a></li-->
            <!--li class="active"><a href="./">Fixed top</a></li-->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

<br/>
<br/>

<br/>
<center><h2 id="top"><b>CoMeDi: <u>Co</u>ntext and <u>Me</u>aning&mdash;Navigating<br/><u>Di</u>sagreements in NLP Annotations</b></h2></center>
<center><h4>Workshop to be held in conjunction with COLING 2025 in Abu Dhabi</h4></center>
<center><h4>January 19, 2025</h4></center>
<br/>

<br/>
<!--div class="alert alert-success" role="alert">
<strong>Update: </strong>We are excited to announce the full workshop program, which you can view <a href="#program">here</a>
</div-->

  <p>Disagreements among annotators pose a significant challenge in Natural Language Processing, impacting the quality and reliability of datasets and consequently the performance of NLP models. This workshop aims to explore the complexities of annotation disagreements, their causes, and strategies towards their effective resolution, with a focus on meaning in context.</p>

  <p>The quality and reliability of annotated data is crucial for the development of robust NLP models. However, managing disagreements among annotators poses significant challenges to researchers and practitioners. Such disagreements can stem from various factors, including subjective interpretations, cultural biases, and ambiguous guidelines. Early research has highlighted the impact of annotator disagreements on data quality and model performance (e.g. Artstein and Poesio, 2008; Pustejovsky and Stubbs, 2012; Plank et al., 2014).</p>

<p>More recent work on perspectivism in NLP, such as that by Basile et al. (2021), highlights the importance of embracing multiple perspectives in annotation tasks to better capture the diversity of human language. This approach argues for the inclusion of various viewpoints to improve the robustness and fairness of NLP models. On the modeling side, various methods for dealing with annotation disagreements have been proposed. For example, Hovy et al. (2013) and Passonneau and Carpenter (2014) identify and weigh annotator reliability to better aggregate contributions, whereas recent approaches following the perspectivism approach leverage inherent disagreements in subjective tasks to train models handling diverse opinions (Davani et al., 2022; Deng et al., 2023).</p>

<a name="speakers"></a>
<div class="page-header">
  <h1>Invited Speaker(s)</h1>
</div>

<ul>
<li><p><h4>Barbara Plank: The Spectrum of Human Label Variation: Reflections from the Last Ten Years in NLP</h4></p></li>
<!--p>
  <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#xxxabs" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </button>
  <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#xxxbio" aria-expanded="false" aria-controls="collapseExample">
    Bio
  </button>
</p>
<div class="collapse" id="xxxabs">
  <div class="card card-body">
    <p>TBD</p>
  </div>
</div>
<div class="collapse" id="xxxbio">
  <div class="card card-body">
    <p>TBD</p>
  </div>
</div-->
</li>
</ul>

<a name="program"></a>
<div class="page-header">
  <h1>Workshop Program</h1>
</div>

<p>
  <table class="table table-striped">
    <tbody>
      <tr>
        <td>9:20</td>
        <td>Opening</td>
      </tr>
      <tr>
        <td class="success">9:30</td>
        <td><b>Invited talk</b>: <i>The Spectrum of Human Label Variation: Reflections from the Last Ten Years in NLP</i><br>Barbara Plank</td>
      </tr>
      <tr>
        <td>10:30</td>
        <td>Coffee break</td>
      </tr>
      <tr>
        <td class="success"></td>
        <td><b>Pre-lunch session</b></td>
      </tr>
      <tr><td class="success">11:00</td><td><i>Is a bunch of words enough to detect disagreement in hateful content?</i></a><br>
Giulia Rizzi, Paolo Rosso and Elisabetta Fersini</td></tr>
      <tr><td class="success">11:20</td><td><i>On Crowdsourcing Task Design for Discourse Relation Annotation</i></a><br>
Frances Yung and Vera Demberg</td></tr>
      <tr><td class="success">11:40</td><td><i>Sources of Disagreement in Data for LLM Instruction Tuning</i></a><br>
Russel Dsouza and Venelin Kovatchev</td></tr>
      <tr><td class="success">12:00</td><td><i>CoMeDi Shared Task: Median Judgment Classification &amp; Mean Disagreement Ranking with Ordinal Word-in-Context Judgments</i></a><br>
Dominik Schlechtweg, Tejaswi Choppa, Wei Zhao and Michael Roth</td></tr>
      <tr><td>12:20</td><td>Lunch break</td></tr>
      <tr><td class="success"/><td><b>Post-lunch session</b></td></tr>
      <tr><td class="success">14:00</td><td><i>Deep-change at CoMeDi: the Cross-Entropy Loss is not All You Need</i></a><br>
Mikhail Kuklin and Nikolay Arefyev</td></tr>
      <tr><td class="success">14:20</td><td><i>Predicting Median, Disagreement and Noise Label in Ordinal Word-in-Context Data</i></a><br>
Tejaswi Choppa, Michael Roth and Dominik Schlechtweg</td></tr>
      <tr><td class="info">14:40</td><td><b>Poster session</b></td></tr>
      <!--tr><td class="info"/><td><i>GRASP at CoMeDi Shared Task: Multi-Strategy Modeling of Annotator Behavior in Multi-Lingual Semantic Judgments</i></a><br>
David Alfter and Mattias Appelgren</td></tr-->
      <tr><td class="info"/><td><s><i>Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from Word-In-Context Perspectives</i></a><br>
Olufunke O. Sarumi, Charles Welch, Lucie Flek and Jörg Schlötterer</td></s></tr>
      <tr><td class="info"/><td><i>JuniperLiu at CoMeDi Shared Task: Models as Annotators in Lexical Semantics Disagreements</i></a><br>
Zhu Liu, Zhen Hu and Ying Liu</td></tr>
      <!--tr><td class="info"/><td><i>MMLabUIT at CoMeDiShared Task: Text Embedding Techniques versus Generation-Based NLI for Median Judgment Classification</i> <sup><a href="https://drive.google.com/file/d/18KEqyV3Twlib5xDunfgtXcHTR8NK5zMr/view">[video]</a></sup><br>
Tai Duc Le and Thin Dang Van</td></tr-->
      <tr><td class="info"/><td><i>ABDN-NLP at CoMeDi Shared Task: Predicting the Aggregated Human Judgment via Weighted Few-Shot Prompting</i></a><br>
Ying Xuan Loke, Dominik Schlechtweg and Wei Zhao</td></tr>
      <tr><td class="info"/><td><i>The Impact of Annotation Choices on Computational Representations of Semantic and Phonological Distance in Sign Languages</i><br>
Lisa Loy</td></tr>
      <tr><td class="info"/><td><i>Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation</i><br>
Zhu Liu and Ying Liu</td></tr>
      <tr><td class="info"/><td><i>Disagreement in Metaphor Annotation of Mexican Spanish Science Tweets</i></a><br>
Alec M. Sanchez-Montero, Gemma Bel-Enguix, Sergio Luis Ojeda Trueba and Gerardo Sierra Martínez</td></tr>
      <tr><td>15:30</td><td>Coffee break</td></tr>
      <tr>
        <td>16:00</td>
        <td>Closing</td>
      </tr>
      <tr><td/><td/></tr>
      <tr>
	<td class="danger"/><td><b>Online only</b></td>
      </tr>
      <tr><td class="danger"/><td><i>Automating Annotation Guideline Improvements using LLMs: A Case Study</i> <sup><a href="https://drive.google.com/file/d/1duRIoUU8tYLgbgr2WSs2pTNFxQUJItvy/view?usp=sharing">[video]</a></sup><br>
Adrien Bibal, Nathaniel Gerlek, Goran Muric, Elizabeth Boschee, Steven C. Fincke, Mike Ross and Steven N. Minton</td></tr>
      <tr><td class="danger"/><td><i>Ambiguity and Disagreement in Abstract Meaning Representation</i> <sup><a href="https://drive.google.com/file/d/1DzrRBY4HcOTiGZX-e8bqQBADPez-RhDf/view?usp=sharing_eil&ts=6786ddc3">[video]</a></sup><br>
Shira Wein</td></tr>
      <tr><td class="danger"/><td><i>FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression</i> <sup><a href="https://drive.google.com/file/d/1Hgufx7EEpPn8bJlNzom5VXf4ai9BvRfG/view">[video]</a></sup><br>
Phuoc Duong Huy Chu</td></tr>     	
      <tr><td class="danger"/><td><i>GRASP at CoMeDi Shared Task: Multi-Strategy Modeling of Annotator Behavior in Multi-Lingual Semantic Judgments</i> <sup><a href="https://drive.google.com/file/d/17PVYUP2InsLO0eevrPYsQulPAakkbaik/view">[video]</a></sup><br>
David Alfter and Mattias Appelgren</td></tr>
      <tr><td class="danger"/><td><i>MMLabUIT at CoMeDi Shared Task: Text Embedding Techniques versus Generation-Based NLI for Median Judgment Classification</i> <sup><a href="https://drive.google.com/file/d/18KEqyV3Twlib5xDunfgtXcHTR8NK5zMr/view">[video]</a></sup><br>
Tai Duc Le and Thin Dang Van</td></tr>
    </tbody>
  </table>
</p>



<a name="call"></a>
<div class="page-header">
  <h1>Call for Submissions</h1>
</div>
<p>We invite both long (8 pages) and short (4 page) papers. The limits refer to
the content and any number of additional pages for references are allowed. The
papers should follow the COLING 2025 formatting instructions.</p>

<p>Each submission must be anonymized, written in English, and contain a title and
abstract. We especially welcome papers that address the following themes, for a single type of disagreement or annotation disagreements in general:</p>

<ul>
  <li>New benchmarks for detecting or categegorizing disagreements</li>
  <li>Models and modeling strategies for variations in annotation</li>
  <li>Evaluation schemes and metrics for phenomena without a single ground truth</li>
  <li>Phenomena that are not yet within reach with current NLP technology.</li>
</ul>

  <p>To encourage discussion and community building and to bootstrap potential collaborations, we elicit, in addition to shared task papers and regular "archival" track papers, also non-archival submissions. These can take two forms:</p>
  
  <ul>
    <li>Works in progress, that are not yet mature enough for a full submission, can be submitted in the form of a title and abstract. Abstracts may be up to two pages in length.</li>
    <li>Already published work, or work currently under submission elsewhere, can be submitted in the form of the original abstract and a copy of the submission/publication.</li>
  </ul>

<p>These works will be reviewed for topical fit and accepted submissions will be presented as posters. Depending on the final workshop program, selected works <i>may</i> be presented in panels. We plan for these to be an opportunity for researchers to present and discuss their work with the relevant community.</p>
      
<p>Please submit your papers <a href="https://www.softconf.com/coling2025/CM-ND-NLP25/">here</a>.</p>


<a name="task"></a>
<div class="page-header">
  <h1>Shared Task</h1>
</div>

<p>The role of linguistic-semantic factors causing disagreements, and the extent to which such cases can potentially be predicted, has been scarcely investigated. Building on previous research on ambiguities arising from pronominal anaphora (Yang et al., 2010) and implicit references (Roth et al., 2022), we will host a shared task on <b>predicting disagreements</b> on word sense annotation in context (a.k.a. <b>Word-in-Context</b>, WiC). Realistic WiC datasets often show considerable disagreement. Consequently, we lose information when discarding instances during aggregation or summarizing them by majority or median judgment. Recent research has started to incorporate this information by using alternative label aggregation methods (Uma et al., 2022; Leonardelli et al., 2023). Modelling this disagreement is important because in a real world scenario we most often do not have clean data. We need to do prediction on samples where high disagreement is expected and which are inherently difficult to categorize. Predicting disagreement can help to detect or filter highly complicated samples.</p>

<p>Participants are asked to solve two subtasks. Both rely on data from human WiC judgments on an ordinal scale, such as the <a href="https://zenodo.org/records/7387261">DWUG EN dataset</a> (Schlechtweg et al., 2021). Each instance has a target word $w$, for which <b>two word uses</b>, $u_1$ and $u_2$, are provided (use pair). Each of these uses expresses a specific meaning of $w$. Annotators were asked to provide labels on an <b>ordinal relatedness scale</b> from 1 (the two uses of the word have completely unrelated meanings) to 4 (the two uses of the word have identical meanings) following the DURel annotation framework (Schlechtweg et al., 2018). As an example, consider the two annotation instances below. Pair (1,2) would likely receive label 4 (identical) while pair (1,3) would rather receive a lower label such as 2 (distantly related).</p>

 <ol start="1">
      <li>...and taking a knife from her pocket, she opened a vein in her little <b>arm</b>.</li>
      <li>...and though he saw her within reach of his <b>arm</b>, yet the light of her eyes seemed as far off.</li>
      <ul>
      <li>Sample judgments: [4,4]; median: 4; mean pairwise difference: 0.0</li>
    </ul>
    </ol>

 <ol start="1">
      <li>...and taking a knife from her pocket, she opened a vein in her little <b>arm</b>.</li>
      <li value="3">It stood behind a high brick wall, its back windows overlooking an <b>arm</b> of the sea which, at low tide, was a black and stinking mud-flat.</li>
      <ul>
      <li>Sample judgments: [2,3,2]; median: 2; mean pairwise difference: 0.667</li>
    </ul>
  </ol>

<h3>Subtask 1: Median Judgment Classification with Ordinal Word-in-Context Judgments (OGWiC) <a name="Subtask1" ></a></h3>

<p>For each use pair $(u_1,u_2)$ participants are asked to predict the median of annotator judgments. This task is similar to the previous WiC (Pilehvar et al., 2019) and GWiC (Armendariz et al., 2020) tasks. However, we limit the label set in predictions and penalize stronger deviations from the true label. This makes OGWiC an <b>ordinal classification task</b> (Sakai, 2021), in contrast to binary classification (WiC) or ranking (GWiC). Predictions will be evaluated against the median labels with the ordinal version of Krippendorff's $\alpha$ (Krippendorff, 2018).
</p>

<p>Treating graded WiC as an ordinal classification task instead of a ranking task constrains model predictions to exactly reproduce instance labels instead of just inferring their relative order. This is advantageous if ordinal labels have an interpretation because predictions then inherit this interpretation. Such an interpretation can be assigned to the DURel scale as explained in Schlechtweg et al. (2018) and in more detail in Schlechtweg (2023, pp. 22-23): Judgment 1-4 can be interpreted as "homonymy" (1), "polysemy" (2), "context variance" (3) and "identity" (4), respectively.
</p>

  
<h3>Subtask 2: Mean Disagreement Ranking with Ordinal Word-in-Context Judgments (DisWiC)<a name="Subtask2" ></a></h3>

<p>For each use pair $(u_1,u_2)$ participants are asked to predict the mean of pairwise absolute judgment differences between annotators: $D(J)=\frac{1}{|J|}\sum_{(j_1,j_2)\in J}(|j_1-j_2|)$, where $J$ is the set of unique pairwise combinations of judgments. For pair (1,2) from above, $D(J)=\frac{1}{2}(|(4-4)|+|(4-4)|)=0.0$ while for (1,3) it amounts to $\frac{1}{3}(|(2-3)|+|(2-2)|+|(3-2)|)=0.667$. DisWiC can be seen as a <b>ranking task</b>. Participants are asked to rank instances according to the magnitude of disagreement observed between annotators. It differs from previous tasks (Leonardelli et al., 2023) by aggregating "gold" labels purely over judgment differences, thus making disagreement the explicit ranking aim. Predictions will be evaluated against the mean disagreement labels with Spearman's $\rho$ (Spearman, 1904).</p>
  
<h3>Organization<a name="OrganizationTask" ></a></h3>
  
<p>Please register to our <a href="https://codalab.lisn.upsaclay.fr/competitions/20077">CodaLab competition</a> and our <a href="https://groups.google.com/g/comedi-shared-task">Google group</a> to participate. (After login into your Google account, you should be able to join the group directly.) The provided starting kits contain training and development data, a description of the data format, the evaluation scripts, baseline scripts and a sample answer/prediction that can be readily uploaded to CodaLab. If you have any questions, please ask them through our Google group.</p>
  
<p>The task is organized by Dominik Schlechtweg, Tejaswi Choppa, Wei Zhao and Michael Roth.</p>


<a name="dates"></a>
<div class="page-header">
  <h1>Important Dates</h1>
</div>

<h3>Shared Task</h3>
<ul>
  <li>August 23, 2024: Release of shared-task training and development data</li>
  <li>August 23, 2024: Start of development phase for Subtask 1</li>
  <li>September 15, 2024: Start of development phase for Subtask 2</li>
  <li>October 14, 2024: Start of evaluation phase for Subtask 1</li>
  <li>October 21, 2024: Start of evaluation phase for Subtask 2</li>
  <li>October 28, 2024: Release of competition results.</li>
  <li>November 18, 2024: Due date for system description papers</li>
  <li>December 4-6, 2024: Author response period</li>
  <li>December 9, 2024: Notification of acceptance</li>
  <li>December 13, 2024: Camera-ready submission deadline</li>
</ul>

Deadlines refer to 11:59pm UTC.


<h3>Regular and non-archival submissions</h3>
<ul> 
  <li>November 22, 2024: Due date for regular and non-archival workshop papers</li>
  <li>December 6, 2024: Notification of acceptance</li>
  <li>December 13, 2024: Camera-ready submission deadline</li>
</ul></li>
</ul>

Deadlines refer to 11:59pm GMT -12 hours ("anywhere in the world").

<a name="organizers"></a>
<div class="page-header">
  <h1>Organizers</h1>
</div>
<ul class="list-unstyled">
<li><a target="_blank" href="https://www.utn.de/en/departments/department-engineering/nlu-lab/">Michael Roth</a>, University of Technology Nuremberg</li>
<li><a target="_blank" href="https://www.ims.uni-stuttgart.de/institut/team/Schlechtweg/">Dominik Schlechtweg</a>, University of Stuttgart</li>
</ul>

<h3>Program Committee<a name="committee" ></a></h3>

<ul class="list-unstyled">
  <li>David Alfter, University of Gothenburg</li>
  <li>Valerio Basile, University of Turin</li>
  <li>Felipe Bravo, University of Chile</li>
  <li>Jing Chen, Hong Kong Polytechnic University</li>
  <li>Diego Frassinelli, University of Konstanz / LMU Munich</li>
  <li>Dubossarsky Haim, Queen Mary University</li>
  <li>Simon Hengchen, iguanodon.ai & Université de Genève</li>
  <li>Snigdha Khanna, Indiana University</li>
  <li>Sandra Kübler, Indiana University</li>
  <li>Andrei Kutuzov, University of Oslo</li>
  <li>Elisa Leonardelli, Fondazione Bruno Kessler</li>
  <li>Melissa Lieffers, Indiana University</li>
  <li>Marie-Catherine de Marneffe, UCLouvain</li>
  <li>Maja Pavlovic, Queen Mary University</li>
  <li>Siyao Peng, LMU Munich</li>
  <li>Pauline Sander, University of Stuttgart</li>
  <li>Pia Sommerauer, Vrije Universiteit Amsterdam</li>
  <li>Alexandra Uma</li>
  <li>Frank D. Zamora-Reina, University of Chile</li>
  <li>Wei Zhao, University of Aberdeen</li>
</ul>

<a name="organizers"></a>
<div class="page-header">
  <h1>References</h1>
</div>

<ul>
  <li>Carlos Santos Armendariz, Matthew Purver, Senja Pollak, Nikola Ljubešić, Matej Ulčar, Ivan Vulić, and Mohammad Taher Pilehvar. 2020. <a href="https://aclanthology.org/2020.semeval-1.3/">SemEval-2020 task 3: Graded word similarity in context</a>. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 36–49, Barcelona (online). International Committee for Computational Linguistics.</li>
  <li>Klaus Krippendorff. 2018. <a href="https://methods.sagepub.com/book/content-analysis-4e">Content Analysis: An Introduction to Its Methodology</a>. SAGE Publications.</li>
  <li>Elisa Leonardelli, Gavin Abercrombie, Dina Almanea, Valerio Basile, Tommaso Fornaciari, Barbara Plank, Verena Rieser, Alexandra Uma, and Massimo Poesio. 2023. <a href="https://aclanthology.org/2023.semeval-1.314/">SemEval-2023 Task 11: Learning with Disagreements (LeWiDi)</a>. In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2304–2318, Toronto, Canada. Association for Computational Linguistics.</li>
  <li>Tetsuya Sakai. 2021. <a href="https://aclanthology.org/2021.acl-long.214/">Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification</a>. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2759–2769, Online. Association for Computational Linguistics.</li>
  <li>Dominik Schlechtweg, Sabine Schulte im Walde, and Stefanie Eckmann. 2018. <a href="https://aclanthology.org/N18-2027/">Diachronic Usage Relatedness (DURel): A Framework for the Annotation of Lexical Semantic Change</a>. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 169–174, New Orleans, Louisiana. Association for Computational Linguistics.</li>
  <li>Dominik Schlechtweg, Nina Tahmasebi, Simon Hengchen, Haim Dubossarsky, and Barbara McGillivray. 2021. <a href="https://aclanthology.org/2021.emnlp-main.567/">DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages</a>. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7079–7091, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</li>
  <li>Dominik Schlechtweg. 2023. <a href="http://dx.doi.org/10.18419/opus-12833">Human and Computational Measurement of Lexical Semantic Change</a>. Ph.D. thesis, University of Stuttgart, Stuttgart, Germany.</li>  
  <li>Charles Spearman. 1904. <a href="https://doi.org/10.2307/1412159">The proof and measurement of association between two things</a>. In American Journal of Psychology, 15(1), 72–101.</li>
  <li>Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019. <a href="https://aclanthology.org/N19-1128/">WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning</a>. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267–1273, Minneapolis, Minnesota. Association for Computational Linguistics.</li>
  <li>Michael Roth, Talita Anthonio, and Anna Sauer. 2022. <a href="https://aclanthology.org/2022.semeval-1.146/">SemEval-2022 task 7: Identifying plausible clari cations of implicit and underspecifed phrases in instructional texts</a>. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 1039–1049, Seattle, United States. Association for Computational Linguistics.</li>
  <li>Alexandra N. Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. 2022. <a href="https://doi.org/10.1613/jair.1.12752">Learning from disagreement: A survey</a>. J. Artif. Int. Res., 72:1385—1470.</li>
  <li>Hui Yang, Anne de Roeck, Alistair Willis, and Bashar Nuseibeh. 2010. <a href="https://aclanthology.org/C10-1137/">A methodology for automatic identification of nocuous ambiguity</a>. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1218–1226, Beijing, China.</li>
</ul>


<hr>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="bootstrap/js/bootstrap.min.js"></script>
<!--<script src="data/leaderboards/leaderboard1.js"></script>
<script src="data/leaderboards/leaderboard2.js"></script>-->

   <div class="footer">
      <div class="container">
        <p class="text-muted">Website by Michael Roth, powered by <a href="http://www.getbootstrap.com">Bootstrap</a></p>
        <br/>
      </div>
    </div>

</div>

</body>
</html>
